common_args:
  training_type: "cross_silo"
  scenario: "horizontal"

model_args:
  model_name_or_path: "Qwen/Qwen3-0.6B"   # ~620 M params
  peft_type: "none"                       # full-parameter finetune

train_args:
  federated_optimizer: "FedAvg"  # Federated learning optimizer
  client_optimizer: "adamw_torch"  # Client-side optimizer
  server_optimizer: "FedAvg"       # Server-side optimizer
  comm_round: 1           # makes FedML's built-in loop exit
  local_num_train_epochs: 0
  local_max_steps: 1      # Run just 1 training step for minimal execution
  client_num_in_total: 1
  client_num_per_round: 1
  output_dir: ".logs/FedML/{run_id}"  # Add output directory for model checkpoints
  # Note: The actual training will be done by scripts/train_grpo_gsm8k.py
  # This config just provides the minimal setup for FedML communication

data_args:
  dataset: "databricks-dolly"  # Dataset identifier for FedML
  dataset_path: []  # Empty list - actual data loading happens in custom script
  dataset_name: "databricks/databricks-dolly-15k"  # Dummy dataset for validation
  test_dataset_size: 100  # Required when dataset has only 1 split
  client_dataset_path: []  # Client-specific dataset path (empty for custom loader)

device_args:
  using_gpu: true

comm_args:
  backend: "MQTT_S3"
  mqtt_config_path: "config/mqtt_config.yaml"
  s3_config_path:  "config/s3_config.yaml" 